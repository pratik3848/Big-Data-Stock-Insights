{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4456fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:12:23 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/03/17 20:12:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/03/17 20:12:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/03/17 20:12:23 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "23/03/17 20:12:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+-------+\n",
      "|              AmpUrl|          ArticleUrl|              author|         description|           ArticleId|            ImageUrl|PublishedUtc|           publisher|             tickers|               title|newsKey|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+-------+\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|The heavy selling...|--1KkCiSDNBqSQsAG...|https://staticx-t...|  2021-12-30|{name -> Zacks In...|              [AXLA]|Down 24% in 4 Wee...| 666825|\n",
      "|https://www.zacks...|https://www.zacks...|Indrajit Bandyopa...|Here we discuss S...|--1z5c4ZCYWJOZ0jJ...|https://staticx-t...|  2022-11-28|{name -> Zacks In...|  [IRMD, TCMD, SWAV]|3 Medical Instrum...|  67088|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|If you are lookin...|--2H9wkkFe2D4j2nL...|https://staticx-t...|  2022-07-20|{name -> Zacks In...|                 [R]|Ryder (R) Is Attr...|1512812|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|The Zacks Consens...|--3F4yKWp_qH7oajd...|https://staticx-t...|  2021-11-16|{name -> Zacks In...|    [CPRT, KAR, IAA]|What's in the Car...|1949643|\n",
      "|https://www.fool....|https://www.fool....|newsfeedback@fool...|Investors are con...|--4ge_EJzjVe2uPdo...|https://g.foolcdn...|  2021-09-13|{name -> The Motl...|              [PRCH]|Why Porch Group S...|2070274|\n",
      "|https://www.globe...|https://www.globe...|HTG Molecular Dia...|TUCSON, Ariz., Ma...|--7B15d2bLL4QKImF...|https://ml.globen...|  2022-05-17|{name -> GlobeNew...|              [HTGM]|HTG Molecular Dia...| 885095|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|In the latest tra...|--7xgMnxd0iqgg8iD...|https://staticx-t...|  2022-04-13|{name -> Zacks In...|               [EPD]|Enterprise Produc...| 733476|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|T-Mobile (TMUS) c...|--B89sztwEGOPvWf_...|https://staticx-t...|  2022-12-27|{name -> Zacks In...|              [TMUS]|T-Mobile (TMUS) G...|1661776|\n",
      "|https://www.globe...|https://www.globe...|Downing Strategic...|                null|--BDhg6TbP5zJ2e3R...|https://www.globe...|  2022-03-18|{name -> GlobeNew...|               [DSM]|DSM: Net Asset Va...| 611711|\n",
      "|https://m.investi...|https://www.inves...|        Jani Ziedins|                null|--C6KDHj6eMhKs55t...|https://i-invdn-c...|  2021-12-09|{name -> Investin...|               [GME]|S&P 500: Trading ...|1515495|\n",
      "|https://www.benzi...|https://www.benzi...|         Bhavik Nair|Meta Platforms In...|--CDwBtj7VlE-UdZn...|https://cdn.benzi...|  2022-09-30|{name -> Benzinga...|[AAPL, V, XOM, META]|Mark Zuckerberg's...|1218142|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|Snowflake Inc. (S...|--GccC8Iw7re5x1Fp...|https://staticx-t...|  2022-01-19|{name -> Zacks In...|              [SNOW]|Snowflake Inc. (S...|1605624|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|The consensus pri...|--GfwW-AuWN1ZV_Ho...|https://staticx-t...|  2022-03-11|{name -> Zacks In...|               [FNA]|Wall Street Analy...|1892782|\n",
      "|https://m.investi...|https://www.inves...|Zacks Investment ...|                null|--GjvnPdrQEkZGOzE...|https://i-invdn-c...|  2021-06-03|{name -> Investin...|[AWK, CWT, WTRG, ...|American Water (A...| 244817|\n",
      "|https://www.benzi...|https://www.benzi...|       Vandana Singh|NeuroMetrix IncÂ (...|--IAJkEV-A6GHYmY-...|https://cdn.benzi...|  2021-10-07|{name -> Benzinga...|              [NURO]|Why Are NeuroMetr...|1982964|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|NetApp's (NTAP) f...|--JKQoorhEAjSaGEO...|https://staticx-t...|  2021-08-26|{name -> Zacks In...|[AMZN, MSFT, NTAP...|NetApp's (NTAP) Q...|1306783|\n",
      "|https://m.investi...|https://www.inves...|    Sunshine Profits|                null|--JZNQy-1-LuSXBmg...|https://i-invdn-c...|  2021-09-17|{name -> Investin...|              [AAPL]|Will Quadruple Wi...|2132654|\n",
      "|https://www.zacks...|https://www.zacks...|Zacks Equity Rese...|Profound Medical ...|--K6bKKmk30ISIA3P...|https://staticx-t...|  2022-08-04|{name -> Zacks In...|        [PROF, ONTX]|Profound Medical ...|1443981|\n",
      "|https://www.globe...|https://www.globe...|  Centerra Gold Inc.|Net Earnings of $...|--KJ5X7fxCL8emuQ4...|https://ml.globen...|  2022-05-04|{name -> GlobeNew...|          [CG, CGAU]|Centerra Gold Rep...| 521286|\n",
      "|             Unknown|https://www.fool....|newsfeedback@fool...|The market downtu...|--LOtgV-cCDDqiMdP...|https://g.foolcdn...|  2022-03-08|{name -> The Motl...|[AMT, TMUS, VZ, DLR]|Want to Double Yo...|1901737|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+--------------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType,\n",
    "                               IntegerType, BooleanType, TimestampType,\n",
    "                              ArrayType, MapType, DateType)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder.appName(\"DimNews\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.24.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#load news DF\n",
    "news_Df = spark \\\n",
    "        .read.option(\"recursiveFileLookup\", \"true\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .parquet(\"gs://stocks-pipeline/raw-data/sock_news\")\n",
    "\n",
    "\n",
    "news_Df = news_Df.withColumn(\"newsKey\", f.row_number().over(Window.orderBy('title')))\n",
    "news_Df=news_Df.dropDuplicates([\"id\"])\n",
    "\n",
    "news_Df = news_Df.drop(\"ticket\")\n",
    "news_Df = news_Df.withColumn('published_utc', f.split(news_Df['published_utc'], 'T').getItem(0))\n",
    "news_Df = news_Df.withColumnRenamed(\"published_utc\",\"PublishedUtc\") \\\n",
    "            .withColumnRenamed(\"amp_url\", \"AmpUrl\") \\\n",
    "            .withColumnRenamed(\"article_url\", \"ArticleUrl\") \\\n",
    "            .withColumnRenamed(\"image_url\", \"ImageUrl\") \\\n",
    "            .withColumnRenamed(\"id\", \"ArticleId\")\n",
    "            \n",
    "news_Df = news_Df.withColumn(\"PublishedUtc\",f.col(\"PublishedUtc\").cast(DateType()))\n",
    "news_Df = news_Df.fillna(value = 'Unknown', subset = [\"AmpUrl\"])\n",
    "\n",
    "persist_df = news_Df.persist()\n",
    "\n",
    "# persist_df = persist_df.drop(\"tickers\")\n",
    "persist_df = persist_df.drop(\"keywords\")\n",
    "\n",
    "print(persist_df.show())\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a41bfe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317760\n"
     ]
    }
   ],
   "source": [
    "print(persist_df.count())\n",
    "# print(persist_df.select(\"id\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "249be929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             tickers|\n",
      "+--------------------+\n",
      "|[SMTC, AI, ASAN, ...|\n",
      "+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sd=persist_df.filter(\"newsKey = 19301\").select(\"tickers\").show()\n",
    "print(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45637560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:47:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:13 WARN org.apache.spark.scheduler.TaskSetManager: Stage 25 contains a task of very large size (5260 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb802eeee0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:47:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:35 WARN org.apache.spark.scheduler.TaskSetManager: Stage 30 contains a task of very large size (5631 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb8a2615e0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:47:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:47:57 WARN org.apache.spark.scheduler.TaskSetManager: Stage 35 contains a task of very large size (6036 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb78637be0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:48:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:48:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:48:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:48:19 WARN org.apache.spark.scheduler.TaskSetManager: Stage 40 contains a task of very large size (5143 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb7883b940>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:48:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:48:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:48:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:48:40 WARN org.apache.spark.scheduler.TaskSetManager: Stage 45 contains a task of very large size (5833 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb8a11c580>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:49:02 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:49:02 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:49:02 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:49:08 WARN org.apache.spark.scheduler.TaskSetManager: Stage 50 contains a task of very large size (5139 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdbc70751f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:49:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:49:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:49:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:49:32 WARN org.apache.spark.scheduler.TaskSetManager: Stage 55 contains a task of very large size (1533 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb77822fd0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "chunk_size = 50000\n",
    "i=0\n",
    "window = Window.orderBy('newsKey')\n",
    "persist_Df = persist_df.withColumn(\"row_idx\", row_number().over(window))\n",
    "\n",
    "\n",
    "# Loop through the dataframe in chunks\n",
    "while i < persist_Df.count():\n",
    "    \n",
    "    # Select the chunk of data\n",
    "    chunk=persist_Df.orderBy('newsKey').filter(persist_Df[\"row_idx\"] >= i).take(chunk_size)\n",
    "\n",
    "    # create a BigQuery client and dataset reference\n",
    "    client = bigquery.Client(project='noted-span-377814')\n",
    "    dataset_ref = client.dataset('Stocks_DW')\n",
    "\n",
    "    # create a BigQuery table and upload the data\n",
    "    table_ref = dataset_ref.table('DimNews')\n",
    "    chunk_df = spark.createDataFrame(chunk).drop(\"row_idx\")\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND')\n",
    "    job = client.load_table_from_dataframe(chunk_df.toPandas(), table_ref, job_config=job_config)\n",
    "    i+=chunk_size\n",
    "    \n",
    "    print(job.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b67571",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9072ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5655902, 2128828)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5655902, 2128828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1642a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 19:59:49 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 19:59:49 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|tickerKey|newsKey|\n",
      "+---------+-------+\n",
      "|      966| 666825|\n",
      "|     5571|  67088|\n",
      "|    10100|  67088|\n",
      "|     9984|  67088|\n",
      "|     8643|1512812|\n",
      "|     2319|1949643|\n",
      "|     5936|1949643|\n",
      "|     5150|1949643|\n",
      "|     8282|2070274|\n",
      "|     5049| 885095|\n",
      "|     3347| 733447|\n",
      "|    10320|1661784|\n",
      "|     2943| 611618|\n",
      "|     4520|1515495|\n",
      "|       29|1218142|\n",
      "|    10781|1218142|\n",
      "|    11449|1218142|\n",
      "|     6670|1218142|\n",
      "|     9616|1605630|\n",
      "|     3961|1892782|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:01:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:30 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb808393d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:01:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb71a09130>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:01:45 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:45 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:45 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb897c0c40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:01:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:51 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb813cbaf0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:01:56 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:56 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:01:56 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb73a2d130>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:01 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:01 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:01 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb78839f70>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb81eb9970>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:14 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:14 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:14 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb765d07f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:19 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:19 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:19 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb7791e7f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:26 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:26 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:26 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb80839a60>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:31 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:31 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:31 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb77b697c0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/17 20:02:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/03/17 20:02:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.job.load.LoadJob object at 0x7fdb74b70460>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fill bridge table\n",
    "\n",
    "bridge_Df_news = news_Df.select(\"newsKey\",\"tickers\").withColumn(\"tickers_exploded\", f.explode(\"tickers\")).drop(\"tickers\")\n",
    "\n",
    "# load tickers DF\n",
    "ticker_Df = spark.read.format(\"parquet\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .load(\"gs://stocks-pipeline/raw-data/ticker_details/*.parquet\")\n",
    "\n",
    "ticker_Df = ticker_Df.withColumn(\"tickerKey\", f.row_number().over(Window.orderBy('ticker')))\n",
    "bridge_Df_ticker = ticker_Df.select(\"tickerKey\", \"ticker\")\n",
    "\n",
    "# join bridge_Df_ticker and bridge_Df_news\n",
    "joined_Df = bridge_Df_ticker \\\n",
    "            .join(bridge_Df_news, bridge_Df_ticker.ticker == bridge_Df_news.tickers_exploded, \"inner\") \\\n",
    "            .select(\"tickerKey\", \"newsKey\")\n",
    "\n",
    "joined_Df = joined_Df.distinct()\n",
    "persist_Df = joined_Df.persist()\n",
    "\n",
    "print(persist_Df.show())\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "chunk_size = 70000\n",
    "i=0\n",
    "window = Window.orderBy('newsKey')\n",
    "persist_Df = persist_Df.withColumn(\"row_idx\", row_number().over(window))\n",
    "\n",
    "\n",
    "# Loop through the dataframe in chunks\n",
    "while i < persist_Df.count():\n",
    "    # Select the chunk of data\n",
    "    chunk=persist_Df.orderBy('newsKey').filter(persist_Df[\"row_idx\"] >= i).take(chunk_size)\n",
    "\n",
    "    # create a BigQuery client and dataset reference\n",
    "    client = bigquery.Client(project='noted-span-377814')\n",
    "    dataset_ref = client.dataset('Stocks_DW')\n",
    "\n",
    "    # create a BigQuery table and upload the data\n",
    "    table_ref = dataset_ref.table('BrgNewsTicker')\n",
    "    chunk_df = spark.createDataFrame(chunk).select('tickerKey','newsKey')\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition='WRITE_APPEND')\n",
    "    job = client.load_table_from_dataframe(chunk_df.toPandas(), table_ref, job_config=job_config)\n",
    "    i+=chunk_size\n",
    "    print(job.result())\n",
    "    \n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca307c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}